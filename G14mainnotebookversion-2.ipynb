{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "#import requests\n",
    "import pandas as pd\n",
    "from skimage import morphology\n",
    "from skimage.color import rgb2hsv\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats.stats import mode\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import seaborn as sns\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cb8835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/theakaroline/Documents/GitHub/Cherry2/ground_truth.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.datasets import load_files\n",
    "\n",
    "image_folder_path = \"/Users/theakaroline/Documents/GitHub/Cherry2/processed_images\"\n",
    "#This folder contains the 100 masked sample images\n",
    "#this should be only the traing data\n",
    "\n",
    "\n",
    "#here the ground Truth shoud be as well.\n",
    "ground_truth = '/Users/theakaroline/Documents/GitHub/Cherry2/ground_truth.csv'\n",
    "# my_data = load_files(data_folder_path, categories =['folder1', 'folder2', 'folder3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba046f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAT_336_705_711_NOBG.png', 'PAT_834_1574_276.png', 'PAT_724_1369_896.png', 'PAT_742_1470_425.png.png', 'PAT_559_1093_774.png', 'PAT_2127_4662_596_NOBG.png', 'PAT_677_1349_541.png', 'PAT_375_763_980.png.png', 'PAT_669_1265_76.png', 'PAT_1022_115_132.png.png', 'PAT_1764_3335_803.png', 'PAT_813_1619_543.png', 'i.png', 'PAT_248_379_695.png.png', 'PAT_82_125_907.png.png', '.DS_Store', 'PAT_93_361_467.png', 'PAT_771_1491_390.png', 'PAT_1304_3507_308.png.png', 'PAT_1414_1433_570.png.png', 'PAT_1946_3924_627_NOBG.png', 'PAT_701_4056_457.png', 'PAT_313_669_935_NOBG.png', 'PAT_1811_3487_560.png', 'PAT_91_356_456.png', 'PAT_2105_4587_204_NOBG.png', 'PAT_952_1807_58.png', 'PAT_821_1547_77.png', 'PAT_359_740_46.png.png', 'PAT_279_1393_118.png.png', 'PAT_762_1436_414.png', 'PAT_796_1509_400.png', 'test_img.png', 'PAT_822_1550_132.png', 'PAT_1315_1110_317.png.png', 'PAT_934_1777_423.png', 'PAT_1441_1531_670.png', 'PAT_823_1558_67.png', 'PAT_1575_2516_23.png.png', 'PAT_247_378_484_NOBG.png', 'PAT_837_1583_124.png', 'PAT_304_653_446_NOBG.png', 'PAT_388_4500_103.png.png', 'PAT_245_376_24_NOBG.png', 'PAT_771_1490_334.png', 'PAT_441_2868_663.png.png', 'PAT_1995_4080_695_NOBG.png', 'PAT_944_1795_666.png', 'PAT_905_1721_327.png', 'PAT_831_1570_637.png', 'PAT_1571_2484_488.png.png', 'PAT_370_2558_175.png.png', 'PAT_1922_3848_451_NOBG.png', 'PAT_288_441_312_NOBG.png', 'PAT_1460_1598_746.png.png', 'PAT_39_55_233.png.png', 'PAT_885_1687_204.png', 'PAT_1810_3484_347.png', 'PAT_388_1590_328.png', 'PAT_1821_3575_920.png', 'PAT_1754_3308_390.png.png', 'PAT_301_649_533_NOBG.png', 'PAT_330_1437_458_NOBG.png', 'PAT_54_83_405.png.png', 'PAT_1174_628_915.png.png', 'PAT_235_360_50.png.png', 'PAT_1874_3687_799.png', 'PAT_679_1286_677.png', 'PAT_1170_609_378.png.png', 'PAT_931_1763_617.png', 'PAT_97_151_503.png', 'PAT_734_1390_631.png', 'PAT_352_729_947_NOBG.png', 'PAT_1027_128_854.png.png', 'PAT_1129_498_930.png.png', 'PAT_87_133_159.png', 'PAT_1492_1705_578.png.png', 'PAT_2034_4269_670_NOBG.png', 'PAT_2124_4650_575_NOBG.png', 'PAT_1414_1432_969.png.png', 'PAT_1306_1086_124.png.png', 'PAT_645_4042_489.png', 'PAT_1026_124_346.png.png', 'PAT_1475_1651_727.png.png', 'PAT_875_1670_121.png', 'PAT_759_1433_973.png.png', 'PAT_987_1859_859.png', 'PAT_2076_4459_450_NOBG.png', 'PAT_1914_3822_863.png', 'PAT_660_1249_469.png', 'PAT_306_1646_971.png.png', 'PAT_228_348_2_NOBG.png', 'PAT_990_1860_636.png', 'PAT_2062_4397_39_NOBG.png', 'PAT_1942_3917_379_NOBG.png', 'PAT_1203_724_762.png']\n",
      "Number of training images: 76\n",
      "Number of testing images: 20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c48093bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BCC</th>\n",
       "      <th>MEL</th>\n",
       "      <th>SCC</th>\n",
       "      <th>ACK</th>\n",
       "      <th>NEV</th>\n",
       "      <th>SEK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PAT_1022_115_132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1026_124_346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1027_128_854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1129_498_930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1170_609_378</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_944_1795_666</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_952_1807_58</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_97_151_503</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_987_1859_859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_990_1860_636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  BCC  MEL  SCC  ACK  NEV  SEK\n",
       "ID                                            \n",
       "PAT_1022_115_132    0    0    0    1    0    0\n",
       "PAT_1026_124_346    0    0    0    1    0    0\n",
       "PAT_1027_128_854    0    0    0    1    0    0\n",
       "PAT_1129_498_930    0    0    0    0    1    0\n",
       "PAT_1170_609_378    0    0    0    1    0    0\n",
       "...               ...  ...  ...  ...  ...  ...\n",
       "PAT_944_1795_666    1    0    0    0    0    0\n",
       "PAT_952_1807_58     1    0    0    0    0    0\n",
       "PAT_97_151_503      1    0    0    0    0    0\n",
       "PAT_987_1859_859    0    0    0    1    0    0\n",
       "PAT_990_1860_636    0    0    1    0    0    0\n",
       "\n",
       "[98 rows x 6 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = os.listdir(image_folder_path)\n",
    "\n",
    "groundTruth_df = pd.read_csv(ground_truth, delimiter = \";\")\n",
    "\n",
    "groundTruth_df = groundTruth_df.set_index(\"ID\")\n",
    "\n",
    "groundTruth_df\n",
    "#groundTruth = groundTruth_df.set_index(\"ID\")\n",
    "#this line chooses the photo ID to be the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "90c01192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need some sort of code to classify the pictures\n",
    "cancerous_mask = groundTruth_df[\"BCC\"] == 1 + (groundTruth_df[\"MEL\"] == 1) + (groundTruth_df[\"SCC\"] == 1)\n",
    "non_cancerous_mask = (groundTruth_df[\"ACK\"] == 1) +(groundTruth_df[\"NEV\"] == 1) + (groundTruth_df[\"SEK\"] == 1)\n",
    "\n",
    "cancerous_mask = pd.DataFrame(cancerous_mask, columns = ['Value'])\n",
    "non_cancerous_mask = pd.DataFrame(non_cancerous_mask, columns = ['Value'])\n",
    "\n",
    "cancerous_mask = cancerous_mask.loc[cancerous_mask.Value, :]\n",
    "non_cancerous_mask = non_cancerous_mask.loc[non_cancerous_mask.Value, :]\n",
    "\n",
    "cancerous_mask_list = cancerous_mask.index.tolist()\n",
    "non_cancerous_mask_list = non_cancerous_mask.index.tolist()\n",
    "\n",
    "\n",
    "# # res = [i for i, val in enumerate(cancerous_mask) if val] \n",
    "# # res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancerous_images = groundTruth_df.index[cancerous_mask].tolist()\n",
    "#non_cancerous_images = groundTruth_df.index[non_cancerous_mask].tolist()\n",
    "# # #So, cancerous_images is a list of image IDs for cancerous skin lesions \n",
    "# # in our GroundTruth dataframe\n",
    "\n",
    "\n",
    "# #path to cancerous images\n",
    "cancerous_with_masks = []\n",
    "for i in cancerous_mask_list:\n",
    "     cancerous_path = os.path.join(image_folder_path, i + \".png\")\n",
    "     cancerous_with_masks.append(cancerous_path)\n",
    "\n",
    "# # #path to non cancerous images\n",
    "non_cancerous_with_masks = []\n",
    "for i in non_cancerous_mask_list:\n",
    "     non_cancerous_path = os.path.join(image_folder_path, i + \".png\")\n",
    "     non_cancerous_with_masks.append(non_cancerous_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336514f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2HEX(color):\n",
    "     return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))\n",
    "\n",
    "#somethis hexcolor is easier, because it is not a tuple but a single line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b844b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_red_scale(image):\n",
    "    # Convert image to HSV color space\n",
    "    #hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bw_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    red = [255, 0, 0]  # RGB\n",
    "    diff = 50\n",
    "    boundaries = [([red[0]-diff, red[1]-diff, red[2]-diff]),     ([red[0]+diff, red[1]+diff, red[2]+diff])]\n",
    "    \n",
    "    lower_red = np.array([1,0,0])\n",
    "    upper_red = np.array([255,110,110])\n",
    "    red_mask = cv2.inRange(rgb_image, lower_red, upper_red)\n",
    "\n",
    "    output_red = rgb_image.copy()\n",
    "    output_red[np.where(red_mask==0)] = 0\n",
    "                         \n",
    "    plt.imshow(output_red)\n",
    "    plt.grid(None)\n",
    "    \n",
    "    #red_mask = cv2.inRange(image, lower_red, upper_red)\n",
    "    # Compute the percentage of red pixels\n",
    "    red_pixel_count = cv2.countNonZero(red_mask)\n",
    "    total_pixels = cv2.countNonZero(mask)\n",
    "    red_percentage = red_pixel_count / total_pixels\n",
    "    print(red_pixel_count)\n",
    "    # Map red percentage to the desired scale\n",
    "    \n",
    "    if red_percentage< 0.05:\n",
    "        red_scale = 0\n",
    "    elif red_percentage < 0.2:\n",
    "        red_scale = 1\n",
    "    elif red_percentage < 0.4:\n",
    "        red_scale = 2\n",
    "    elif red_percentage < 0.6:\n",
    "        red_scale = 3\n",
    "    else:\n",
    "        red_scale = 4\n",
    "    return red_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yellow_scale(image):\n",
    "    # Convert image to HSV color space\n",
    "    #hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bw_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    yellow = [255, 255, 0]  # RGB\n",
    "    diff = 50\n",
    "    boundaries = [([yellow[0]-diff, yellow[1]-diff, yellow[2]-diff]),\n",
    "                  ([yellow[0]+diff, yellow[1]+diff, yellow[2]+diff])]\n",
    "    \n",
    "    lower_yellow = np.array([220,120,120])\n",
    "    upper_yellow = np.array([255,255,255])\n",
    "    yellow_mask = cv2.inRange(rgb_image, lower_yellow, upper_yellow)\n",
    "\n",
    "    output_yellow = rgb_image.copy()\n",
    "    output_yellow[np.where(yellow_mask==0)] = 0\n",
    "                         \n",
    "    plt.imshow(output_yellow)\n",
    "    plt.grid(None)\n",
    "    \n",
    "    #red_mask = cv2.inRange(image, lower_red, upper_red)\n",
    "    # Compute the percentage of red pixels\n",
    "    yellow_pixel_count = cv2.countNonZero(yellow_mask)\n",
    "    total_pixels = cv2.countNonZero(mask)\n",
    "    yellow_percentage = yellow_pixel_count / total_pixels\n",
    "    print(yellow_pixel_count)\n",
    "    # Map red percentage to the desired scale\n",
    "    \n",
    "    if yellow_percentage < 0.05:\n",
    "        yellow_scale = 0\n",
    "    elif yellow_percentage < 0.2:\n",
    "        yellow_scale = 1\n",
    "    elif yellow_percentage < 0.4:\n",
    "        yellow_scale = 2\n",
    "    elif yellow_percentage < 0.6:\n",
    "        yellow_scale = 3\n",
    "    else:\n",
    "        yellow_scale = 4\n",
    "    return yellow_scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc88a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brown_scale(image):\n",
    "    # Convert image to HSV color space\n",
    "    #hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bw_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    brown = [255, 255, 0]  # RGB\n",
    "    diff = 50\n",
    "    boundaries = [([brown[0]-diff, brown[1]-diff, brown[2]-diff]),\n",
    "                  ([brown[0]+diff, brown[1]+diff, brown[2]+diff])]\n",
    "    \n",
    "    lower_brown = np.array([50,0,0])\n",
    "    upper_brown= np.array([150,200,100])\n",
    "    brown_mask = cv2.inRange(rgb_image, lower_brown, upper_brown)\n",
    "\n",
    "    output_brown = rgb_image.copy()\n",
    "    output_brown[np.where(brown_mask==0)] = 0\n",
    "                         \n",
    "    plt.imshow(output_brown)\n",
    "    plt.grid(None)\n",
    "    \n",
    "    #red_mask = cv2.inRange(image, lower_red, upper_red)\n",
    "    # Compute the percentage of red pixels\n",
    "    brown_pixel_count = cv2.countNonZero(brown_mask)\n",
    "    total_pixels = cv2.countNonZero(bw_image)\n",
    "    brown_percentage = brown_pixel_count / total_pixels\n",
    "    print(brown_pixel_count)\n",
    "    # Map red percentage to the desired scale\n",
    "    \n",
    "    if brown_percentage < 0.05:\n",
    "        brown_scale = 0\n",
    "    elif brown_percentage < 0.2:\n",
    "        brown_scale = 1\n",
    "    elif brown_percentage < 0.4:\n",
    "        brown_scale = 2\n",
    "    elif brown_percentage < 0.6:\n",
    "        brown_scale = 3\n",
    "    else:\n",
    "        brown_scale = 4\n",
    "    return brown_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_purple_scale(image):\n",
    "    # Convert image to HSV color space\n",
    "    #hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bw_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    purple = [255, 255, 0]  # RGB\n",
    "    diff = 50\n",
    "    boundaries = [([purple[0]-diff, purple[1]-diff, purple[2]-diff]),\n",
    "                  ([purple[0]+diff, purple[1]+diff, purple[2]+diff])]\n",
    "    \n",
    "    lower_purple = np.array([50,99,0])\n",
    "    upper_purple = np.array([130,100,255])\n",
    "    purple_mask = cv2.inRange(rgb_image, lower_purple, upper_purple)\n",
    "\n",
    "    output_purple = rgb_image.copy()\n",
    "    output_purple[np.where(purple_mask==0)] = 0\n",
    "                         \n",
    "    plt.imshow(output_purple)\n",
    "    plt.grid(None)\n",
    "    \n",
    "    #red_mask = cv2.inRange(image, lower_red, upper_red)\n",
    "    # Compute the percentage of red pixels\n",
    "    purple_pixel_count = cv2.countNonZero(purple_mask)\n",
    "    total_pixels = cv2.countNonZero(bw_image)\n",
    "    purple_percentage = purple_pixel_count / total_pixels\n",
    "    print(purple_pixel_count)\n",
    "    # Map red percentage to the desired scale\n",
    "    \n",
    "    if purple_percentage < 0.05:\n",
    "        purple_scale = 0\n",
    "    elif purple_percentage < 0.2:\n",
    "        purple_scale = 1\n",
    "    elif purple_percentage < 0.4:\n",
    "        purple_scale = 2\n",
    "    elif purple_percentage < 0.6:\n",
    "        purple_scale = 3\n",
    "    else:\n",
    "        purple_scale = 4\n",
    "    return purple_scale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff685df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_reader(img):\n",
    "    im = cv2.imread(img)\n",
    "\n",
    "#     #we have to get the coordinates of every pixel in the image so\n",
    "     xy_coords = np.flip(np.column_stack(np.where(im >= 0)), axis=1)\n",
    "    \n",
    "    #     This line calculates the coordinates of every pixel in the image where the pixel values are greater than or equal to 0. \n",
    "    #  Here's a step-by-step breakdown of what happens:\n",
    "\n",
    "    # [Step 1] np.where(im >= 0) returns the indices where the condition im >= 0 is satisfied. It returns a tuple of arrays,\n",
    "    #  one for each dimension of the image. \n",
    "\n",
    "    # [Step 2]np.column_stack combines the indices into columns. \n",
    "    # So, if the image has shape (height, width, channels), then the result will have shape (num_pixels, num_dimensions), \n",
    "    # where num_dimensions is the number of dimensions in the image (usually 3 for RGB images).\n",
    "\n",
    "    # [Step 3]np.flip is used to reverse the order of columns in the resulting array.\n",
    "\n",
    "     a_del = np.delete(xy_coords, 0, 1)\n",
    "     a_del = a_del[::3][:, [0, 1]]\n",
    "    \n",
    "#    These lines manipulate the xy_coords array to prepare it for further processing:\n",
    "\n",
    "    # [Step 1] np.delete(xy_coords, 0, 1) deletes the first column of xy_coords (which represents the color channel index, \n",
    "    # i.e., 0 for red, 1 for green, 2 for blue).\n",
    "\n",
    "    # [Step 2]a_del[::3] selects every third row from a_del because each pixel has three color coordinates (RGB), \n",
    "    # so we only need one set of coordinates for each pixel.\n",
    "\n",
    "    # [Step 3][:, [0, 1]] selects the first two columns of the resulting array, \n",
    "    # representing the x and y coordinates of each pixel. This is done using array indexing.\n",
    "\n",
    "\n",
    "     image = Image.fromarray(im)\n",
    "     rgb_image = image.convert('RGB')\n",
    "    \n",
    "    #  These lines convert the image from the OpenCV format (im) to an Image object (image).\n",
    "    #  Then, the convert method is used to convert the image to the RGB color mode,\n",
    "    #  creating a new Image object called rgb_image.\n",
    "\n",
    "     rgb1 = [rgb_image.getpixel((int(i[0]),int(i[1]))) for i in a_del]\n",
    "    \n",
    "    #    This line retrieves the RGB values for each coordinate in a_del by calling the getpixel method of rgb_image. \n",
    "    # The getpixel method takes a tuple of coordinates (x, y) and returns the RGB values at that pixel location. \n",
    "    # The resulting RGB values are stored in the rgb1 list.\n",
    "\n",
    "     dd = [RGB2HEX(i) for i in rgb1]\n",
    "     ss = list(set(dd))\n",
    "#   \n",
    "     counter_colours =  Counter(dd)\n",
    "#     #popping the black color\n",
    "     counter_colours.pop('#000000')\n",
    "     #print(counter_colours)\n",
    "#     #plt.imshow(img)\n",
    "     return counter_colours\n",
    "\n",
    "#dont know if we should use this code or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv(img):    \n",
    "    im = cv2.imread(img)\n",
    "    #im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    new_arr_no_0 = im[np.where(im!=0)]\n",
    "    image_copy = im.copy()\n",
    "    non_black_pixels_mask = np.any(im != [0, 0, 0], axis=-1)  \n",
    "    no_black = image_copy[non_black_pixels_mask]\n",
    "    hsv_image = rgb2hsv(no_black)\n",
    "    min_max = [np.amax(hsv_image[:,1]) - np.amin(hsv_image[:,1])]\n",
    "    print(np.amin(hsv_image[:,1]))\n",
    "    return min_max\n",
    "\n",
    "# this code is very cool actually\n",
    "\n",
    "\"\"\"\n",
    "A high difference in saturation means that there is a \n",
    "large range of color intensity in the image, so some \n",
    "parts of the image may have very intense colors while \n",
    "others have muted colors.\n",
    "\n",
    "in our case, a picture with a high saturation (output close to 1) means\n",
    " that there is a big variety in the color. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "539280ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetry_extraction(image):\n",
    "    #01.read the image using the Matplotlib library \n",
    "    image = plt.imread(image)\n",
    "\n",
    "    #02.Crop the image : this is done by finding the indices of the non zero elements(pixels with non zero intensity values)\n",
    "    #in order to do that,here we use np.nonezero from the numpy library \n",
    "    #center of the  shape is the center of the image\n",
    "    #the boarders of the shape are the boaders of the image\n",
    "    y_nonzero, x_nonzero = np.nonzero(image)\n",
    "    image = image[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)] #slice the image to include only the non zero regions\n",
    "\n",
    "    #03.Cut the image in halves\n",
    "    #04.Find the point of cutoff\n",
    "    height, width = image.shape\n",
    "    cutoff_of_width = width//2\n",
    "    cutoff_of_height  = height // 2\n",
    "\n",
    "    #05.Cut the image vertically and horizontally in two\n",
    "    vertical1_im = image[:,:cutoff_of_width]\n",
    "    vertical2_im = image[:,cutoff_of_width:]\n",
    "    horizontal1_im = image[:cutoff_of_height,:]\n",
    "    horizontal2_im = image[cutoff_of_height:,:]\n",
    "\n",
    "    #Simply put, in step 03, 04 and 05, cuts the cropped image into four sections: top, bottom , left and right.\n",
    "    #This is done by dividing the width and height of the image by 2 and using the resulting values to slice the image horizontally and vertically\n",
    "\n",
    "\n",
    "    #06.Flip one of the images both vertically and horizontally to create mirror images for each of the four sections.\n",
    "    # This is done by NumPy array indexing with negative  step size to reverse the order of elements along the specified axis. \n",
    "    vertical_indexer = [slice(None)]*vertical2_im.ndim\n",
    "    horizontal_indexer = [slice(None)]*horizontal2_im.ndim\n",
    "    #Here, slice(None) creates a slice object that includes all elements along the specified axis, and imVertical2.ndim and imHorizontal2.ndim retrun the number of dimensions of the respective arrays.\n",
    "    #create two lists of slice objects with the same lenght as the number of dimesnions of the corresponding arrays.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    vertical_indexer[1]= slice(None,None, -1)\n",
    "    vertical_indexer[0]=slice(None, None,-1)\n",
    "    #modify the slice objects in the second position  of each list to include all elements alng the corresponding axis in reverse order, using the -1 step size.\n",
    "    #This flips the array along that axis effectively\n",
    "\n",
    "\n",
    "\n",
    "    vertical2_im = vertical2_im[tuple(vertical_indexer)]\n",
    "    horizontal2_im = horizontal2_im[tuple(horizontal_indexer)]\n",
    "    #convert each list of slice objects to tuple and use it for NumPy indexing to obtain the flipped images.\n",
    "\n",
    "\n",
    "\n",
    "    #07. if the images don't have the same shape, cut the biggest image\n",
    "    #This can happen if the shape of the original shape was an odd number\n",
    "    vertical2_im =vertical2_im[0:vertical1_im.shape[0], 0:vertical1_im.shape[1]]\n",
    "    horizontal2_im = horizontal2_im[0:horizontal1_im.shape[0], 0:horizontal1_im.shape[1]]\n",
    "\n",
    "    im_bwx_vertical = cv2.bitwise_xor(vertical1_im,vertical2_im)\n",
    "    im_bwx_horizontal = cv2.bitwise_xor(horizontal1_im, horizontal2_im)\n",
    "\n",
    "    vertical_area = np.sum(im_bwx_vertical ==1)\n",
    "    horizontal_area= np.sum(im_bwx_horizontal==1)\n",
    "    mean_area = (vertical_area + horizontal_area) //2\n",
    "    #np.sum is used to count the number of white pixels in each binary image im_bwx_vertical and im_bwx_horizontal. \n",
    "    #This count is performed using the logical expresion im_bwx_vertical == 1 and im_bwx_horizontal == 1 respectively. The expression returns a Boolean array that is True where the value of im_bwx_vertical or im_bwx_horizontal is equal to 1, and False elsewhere. The np.sum() function then counts the number of True values in the array and returns the total count of white pixels in each image.\n",
    "\n",
    "\n",
    "    #Here, the asymmetry level is calculated as a percentage of the non zero pixels  in the overlapped image over the lesion area\n",
    "    return(mean_area/np.sum(im==1))*100\n",
    "    #np.sum(im==1) is used to count the toatal number of non zero( while) pixels in the original image\n",
    "    #(mean_area/np.sum(im==1))*100 calculates the asymmetry level of the  input image as a percentage.\n",
    "    #the numerator is the average area of asymmetry  between two halves of the input image, in pixels.\n",
    "    # The denominator us the total number of non zero pixels in the input image.\n",
    "    #The result is  then multiplied by 100  to convert the values in to percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb902787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perimeter_area(im):\n",
    "    image = plt.imread(im)\n",
    "    mask = image.copy() #creates a copy of the original image, which will be used to create a binary mask\n",
    "    area = np.sum(mask) #calculates the area of the lesion by summing the values of all pixels in the binary mask\n",
    "    struct_el = morphology.disk(1) \n",
    "    \n",
    "    #cretaes  a disk shaped structuring element with a radius of 1, which wil be used for binary erosion.\n",
    "    #Binary erosion is a morphological operation used in image processing to reduce the size of foreground objects(usually represented as white pixels) and eliminate isolated pixels or small components\n",
    "    #The operation is performed by sliding a small binary structuring element (also known as kernel or mask) over the image and checking if all the pixels in the element match the corresponding pixels in the image.\n",
    "    #If there is at least one mismatch, the pixel in the center of the structuring element is set to 0 (black), otherwise it is set to 1 (white).\n",
    "    #As a result, the foreground pixels in the input image that are not completely covered by the structuring element are removed in the output image.\n",
    "    # The size of the foreground objects is thus reduced, while their shape is preserved as much as possible.\n",
    "    #The process is repeated iteratively until the foreground objects can no longer be further reduced, or until a desired level of erosion has been achieved.\n",
    "    \n",
    "    mask_eroded = morphology.binary_erosion(mask, struct_el) #erodes the binary mask using the strucutring element, resulting in a new binary mask with the same shape as the original mask but with the edges erroded.\n",
    "    image_perimeter = mask - mask_eroded  #subtracts the eroded mask from the original mask to obtain an image that contains only the perimeter of the lesion.\n",
    "    perimeter = np.sum(image_perimeter) #caculates the perimeter of the lesion by summing the values of all pixels in the perimeter image.\n",
    "    return [area, perimeter] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and testing data : trying to split the data set into training data and  testing data\n",
    "# import matplotlib.pyplot as plot\n",
    "# from sklearn.datasets import load_files\n",
    "\n",
    "# data_folder_path = '/Users/nuvanjanashashipraba/Desktop/Images'\n",
    "# my_data = load_files(data_folder_path, categories =['folder1', 'folder2', 'folder3']) # loading the data from the folder path specified in step 3 and categorizing them into tthree folders\n",
    "\n",
    "# X = my_data.data #assigning the image data to X\n",
    "# Y = my_data.target #assigning the target labels to Y\n",
    "\n",
    "# from sklearn.model_selection import train_test_split #importing the train_test_split function from the sklearn.model _slection library for splitting the data into trainin and testing sets.\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 2)\n",
    "# #splitting the data into training and testing sets, with 20% of the data set aside for testing and setting the random state for reproducibility.\n",
    "#The resulting four variables represent the training and testing data for both the image data.\n",
    "\n",
    "#print(\"The dimensions of the features of training data \")\n",
    "#print(X_train.shape)\n",
    "#print(\"The dimensions of the features of test data \")\n",
    "#print(X_test.shape)\n",
    "#print(\"The dimensions of the target values of training data \")\n",
    "#print(Y_train.shape)\n",
    "#print(\"The dimensions of the target values of test data \")\n",
    "#print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd1ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_prediction(k, train, classes, test): #classes : the class labels of the training data.\n",
    "    neighb = KNeighborsClassifier(n_neighbors= k)\n",
    "    neighb.fit(train, classes.ravel())\n",
    "    classes_pred = neighb.predict(test)\n",
    "    return classes_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cac41648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(k,train,classes,test,classes_test):\n",
    "    performance = []\n",
    "    perform1 =[]\n",
    "    #the 'performance' list stores the predicted lables for each value of k\n",
    "    #the 'perform1' list stores the accuracy score for each value of k\n",
    "\n",
    "    for i in range(1,k):\n",
    "        a = make_knn_prediction(i,train,classes,test)\n",
    "        performance.append(a)\n",
    "        for j in performance:\n",
    "            b = accuracy_score(classes_test,j) #'accuracy_score' is function from scikit learn \n",
    "        perform1.append(b)\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.plot(perform1)\n",
    "    plt.title(\"Classification Accuracy of KNN for Different Values of k\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.xlabel(\"Value of k\");\n",
    "    plt.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "    return np.mean(perform1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a672c997",
   "metadata": {},
   "source": [
    "### Splittting the data set for the color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ffcb22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nuvanjanashashipraba/Documents/GitHub/cherry2/processed_images\n"
     ]
    }
   ],
   "source": [
    "#checking the path for the folder\n",
    "import os\n",
    "\n",
    "# Get the path to the images folder\n",
    "folder_name = \"processed_images\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "# Print the path\n",
    "print(folder_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f22cc99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ec9e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 76\n",
      "Number of testing images: 20\n"
     ]
    }
   ],
   "source": [
    "#training and testing data : trying to split the data set into training data and  testing data\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "\n",
    "data_folder_path = '/Users/theakaroline/Documents/GitHub/cherry2/processed_images'\n",
    "\n",
    "\n",
    "img_files = list(os.listdir(image_folder_path))\n",
    "\n",
    "\n",
    "train_files, test_files = train_test_split(img_files, test_size=0.2, random_state = 2)\n",
    "\n",
    "# Print number of images in each set\n",
    "print('Number of training images:', len(train_files))\n",
    "print('Number of testing images:', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84225b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_data = my_data.to_numpy() # what is the df_features_extra in our case? what is the \"Nb_of-colours\" in our case?\n",
    "# #df_features = the name of the csv or the varibale name that you used to read the csv file or the data set\n",
    "# #because our data set is not a csv  file, can we just write: name of the dataset.to_numpy()\n",
    "\n",
    "# color_train, color_test, clas_train, clas_test = train_test_split(color_data, clas, test_size = 0.6, random_state= 4)\n",
    "\n",
    "# color_train = color_train. reshape(-1, 1)\n",
    "# clas_train = clas_train.reshape (-1, 1)\n",
    "# color_test = color_test.reshape( -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_prediction(7, color_train, clas_train, color_test))\n",
    "print(clas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec75f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test(380,color_train, clas_train, color_test, clas_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea7f3045",
   "metadata": {},
   "source": [
    "### Splitting the data set for the Saturation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5acb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data = df_features_extra[\"sat_value\"].to_numpy()\n",
    "sat_train, sat_test, clas_train, clas_test = train_test_split(sat_data,clas,test_size=0.6, random_state=4)\n",
    "\n",
    "sat_train = sat_train,reshape(-1,1)\n",
    "clas_train = clas_train.reshape(-1,1)\n",
    "sat_test = sat_test.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_prediction(7,sat_train, clas_train,sat_test))\n",
    "print(clas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26007a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test(150, sat_train,clas_train,sat_test,clas_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e936baf",
   "metadata": {},
   "source": [
    "### Splitting the data set for the Asymmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splittng the data set for the Asymmetry\n",
    "aymmetry_data =df_features_extra [\"Aymmetry\"].to_numpy() #but what is the df_features_extra in our case? # what is the \"Asymmetry\" in our case.\n",
    "#df_features = the name of the csv or the varibale name that you used to read the csv file or the data set\n",
    "#because our data set is not a csv  file, can we just write: name of the dataset.to_numpy()\n",
    "asymmetry_train = asymmetry_test, clas_train, clas_test = train_test_split(asymmetry_data, clas, test_size = 0.6, random_state= 4)\n",
    "asymmetry_train = asymmetry_train.reshape(-1,1)\n",
    "clas_train = clas_train.reshape(-1, 1)\n",
    "asymmetry_test = asymmetry_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f324940",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_prediction(7,asymmetry_train, clas_train,asymmetry_test))\n",
    "print(clas_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test(800,asymmetry_train,clas_train,asymmetry_test.clas_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
